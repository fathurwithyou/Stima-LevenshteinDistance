{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74198dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a729c2ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import requests\n",
    "import re\n",
    "import time\n",
    "from collections import Counter\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, balanced_accuracy_score\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from scipy import stats\n",
    "\n",
    "\n",
    "import string\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "\n",
    "\n",
    "plt.style.use(\"seaborn-v0_8\")\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201f69a4",
   "metadata": {},
   "source": [
    "## Indonesian Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c833d3fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_comprehensive_indonesian_stopwords():\n",
    "    \"\"\"\n",
    "    Get comprehensive Indonesian stop words categorized by type\n",
    "    \"\"\"\n",
    "    stopwords_dict = {\n",
    "        \"pronouns\": [\n",
    "            \"saya\",\n",
    "            \"aku\",\n",
    "            \"kamu\",\n",
    "            \"anda\",\n",
    "            \"dia\",\n",
    "            \"ia\",\n",
    "            \"kita\",\n",
    "            \"kami\",\n",
    "            \"kalian\",\n",
    "            \"mereka\",\n",
    "            \"ku\",\n",
    "            \"mu\",\n",
    "            \"nya\",\n",
    "            \"sih\",\n",
    "            \"dong\",\n",
    "        ],\n",
    "        \"conjunctions\": [\n",
    "            \"dan\",\n",
    "            \"atau\",\n",
    "            \"tetapi\",\n",
    "            \"namun\",\n",
    "            \"serta\",\n",
    "            \"bahkan\",\n",
    "            \"sedangkan\",\n",
    "            \"padahal\",\n",
    "            \"karena\",\n",
    "            \"sebab\",\n",
    "            \"akibat\",\n",
    "            \"hingga\",\n",
    "            \"sampai\",\n",
    "            \"supaya\",\n",
    "            \"agar\",\n",
    "            \"meski\",\n",
    "            \"walaupun\",\n",
    "            \"kendati\",\n",
    "            \"biarpun\",\n",
    "        ],\n",
    "        \"prepositions\": [\n",
    "            \"di\",\n",
    "            \"ke\",\n",
    "            \"dari\",\n",
    "            \"pada\",\n",
    "            \"untuk\",\n",
    "            \"dengan\",\n",
    "            \"oleh\",\n",
    "            \"dalam\",\n",
    "            \"atas\",\n",
    "            \"bawah\",\n",
    "            \"depan\",\n",
    "            \"belakang\",\n",
    "            \"samping\",\n",
    "            \"antara\",\n",
    "            \"diantara\",\n",
    "            \"melalui\",\n",
    "            \"menuju\",\n",
    "            \"terhadap\",\n",
    "            \"tentang\",\n",
    "            \"mengenai\",\n",
    "        ],\n",
    "        \"determiners\": [\n",
    "            \"ini\",\n",
    "            \"itu\",\n",
    "            \"yang\",\n",
    "            \"para\",\n",
    "            \"sang\",\n",
    "            \"si\",\n",
    "            \"sebuah\",\n",
    "            \"suatu\",\n",
    "            \"beberapa\",\n",
    "            \"semua\",\n",
    "            \"seluruh\",\n",
    "            \"setiap\",\n",
    "            \"masing\",\n",
    "            \"tiap\",\n",
    "        ],\n",
    "        \"auxiliaries\": [\n",
    "            \"adalah\",\n",
    "            \"ialah\",\n",
    "            \"yaitu\",\n",
    "            \"yakni\",\n",
    "            \"akan\",\n",
    "            \"telah\",\n",
    "            \"sudah\",\n",
    "            \"sedang\",\n",
    "            \"tengah\",\n",
    "            \"pernah\",\n",
    "            \"bisa\",\n",
    "            \"dapat\",\n",
    "            \"boleh\",\n",
    "            \"harus\",\n",
    "            \"wajib\",\n",
    "            \"perlu\",\n",
    "            \"mau\",\n",
    "            \"ingin\",\n",
    "            \"hendak\",\n",
    "        ],\n",
    "        \"common_words\": [\n",
    "            \"ada\",\n",
    "            \"tidak\",\n",
    "            \"bukan\",\n",
    "            \"belum\",\n",
    "            \"juga\",\n",
    "            \"saja\",\n",
    "            \"hanya\",\n",
    "            \"masih\",\n",
    "            \"lagi\",\n",
    "            \"pula\",\n",
    "            \"pun\",\n",
    "            \"lah\",\n",
    "            \"kah\",\n",
    "            \"tah\",\n",
    "            \"deh\",\n",
    "            \"kok\",\n",
    "            \"sih\",\n",
    "            \"ya\",\n",
    "            \"iya\",\n",
    "            \"oh\",\n",
    "            \"eh\",\n",
    "            \"ah\",\n",
    "            \"wah\",\n",
    "            \"aduh\",\n",
    "            \"duh\",\n",
    "        ],\n",
    "        \"question_words\": [\n",
    "            \"apa\",\n",
    "            \"siapa\",\n",
    "            \"kapan\",\n",
    "            \"dimana\",\n",
    "            \"kemana\",\n",
    "            \"darimana\",\n",
    "            \"mengapa\",\n",
    "            \"kenapa\",\n",
    "            \"bagaimana\",\n",
    "            \"berapa\",\n",
    "            \"mana\",\n",
    "        ],\n",
    "        \"time_indicators\": [\n",
    "            \"hari\",\n",
    "            \"bulan\",\n",
    "            \"tahun\",\n",
    "            \"minggu\",\n",
    "            \"jam\",\n",
    "            \"menit\",\n",
    "            \"detik\",\n",
    "            \"waktu\",\n",
    "            \"masa\",\n",
    "            \"saat\",\n",
    "            \"ketika\",\n",
    "            \"sewaktu\",\n",
    "            \"semasa\",\n",
    "            \"selama\",\n",
    "            \"sambil\",\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    all_stopwords = set()\n",
    "    for category, words in stopwords_dict.items():\n",
    "        all_stopwords.update(words)\n",
    "\n",
    "    return all_stopwords, stopwords_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be2663be",
   "metadata": {},
   "source": [
    "## Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5cec6f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "class settings:\n",
    "    LABEL = \"sentiment\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f8d6f4d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ketahui informasi pembagian #PPKM di wilayah J...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tempat Ibadah di Wilayah PPKM Level 1 Boleh Be...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Juru bicara Satgas Covid-19, Wiku Adisasmito m...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ketahui informasi pembagian #PPKM di wilayah J...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Kementerian Agama menerbitkan Surat Edaran Nom...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  sentiment\n",
       "0  Ketahui informasi pembagian #PPKM di wilayah J...          1\n",
       "1  Tempat Ibadah di Wilayah PPKM Level 1 Boleh Be...          1\n",
       "2  Juru bicara Satgas Covid-19, Wiku Adisasmito m...          1\n",
       "3  Ketahui informasi pembagian #PPKM di wilayah J...          1\n",
       "4  Kementerian Agama menerbitkan Surat Edaran Nom...          1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(r\"data\\INA_TweetsPPKM_Labeled_Pure.csv\", sep=\"\\t\")\n",
    "df = df[[\"Tweet\", \"sentiment\"]]\n",
    "df.rename(columns={\"Tweet\": \"text\", \"sentiment\": settings.LABEL}, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e7007289",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_regex_patterns():\n",
    "    \"\"\"\n",
    "    Create comprehensive regex patterns for Indonesian text cleaning\n",
    "    \"\"\"\n",
    "    patterns = {\n",
    "        \"urls\": r\"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\",\n",
    "        \"mentions\": r\"@[A-Za-z0-9_]+\",\n",
    "        'hashtags': r'#',\n",
    "        \"phone_numbers\": r\"\\b\\d{4,}\\b\",\n",
    "        \"emails\": r\"\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b\",\n",
    "        \"repeated_chars\": r\"(.)\\1{2,}\",  \n",
    "        \"numbers\": r\"\\b\\d+\\b\",\n",
    "        \"special_chars\": r\"[^a-zA-Z\\s]\",\n",
    "        \"extra_whitespace\": r\"\\s+\",\n",
    "        \"short_words\": r\"\\b\\w{1,2}\\b\",  \n",
    "        \"repeated_words\": r\"\\b(\\w+)(?:\\s+\\1\\b)+\",  \n",
    "        \"emoticons\": r\"[:-;=][oO\\-]?[D\\)\\]\\(\\[\\\\OpP]\",\n",
    "        \"single_chars\": r\"\\b[a-zA-Z]\\b\",\n",
    "    }\n",
    "\n",
    "    return patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb2cc56d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing with strategy = standard\n",
      "\n",
      "--- Preview of standard preprocessing results ---\n",
      "                                                text  \\\n",
      "0  Ketahui informasi pembagian #PPKM di wilayah J...   \n",
      "1  Tempat Ibadah di Wilayah PPKM Level 1 Boleh Be...   \n",
      "2  Juru bicara Satgas Covid-19, Wiku Adisasmito m...   \n",
      "3  Ketahui informasi pembagian #PPKM di wilayah J...   \n",
      "4  Kementerian Agama menerbitkan Surat Edaran Nom...   \n",
      "\n",
      "                             processed_text_standard  \n",
      "0  ketahui informasi pembagian ppkm wilayah jabar...  \n",
      "1  tempat ibadah wilayah ppkm level berkapasitas ...  \n",
      "2  juru bicara satgas covid wiku adisasmito menje...  \n",
      "3  ketahui informasi pembagian ppkm wilayah jabar...  \n",
      "4  kementerian agama menerbitkan surat edaran nom...  \n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Processing with strategy = aggressive\n",
      "\n",
      "--- Preview of aggressive preprocessing results ---\n",
      "                                                text  \\\n",
      "0  Ketahui informasi pembagian #PPKM di wilayah J...   \n",
      "1  Tempat Ibadah di Wilayah PPKM Level 1 Boleh Be...   \n",
      "2  Juru bicara Satgas Covid-19, Wiku Adisasmito m...   \n",
      "3  Ketahui informasi pembagian #PPKM di wilayah J...   \n",
      "4  Kementerian Agama menerbitkan Surat Edaran Nom...   \n",
      "\n",
      "                           processed_text_aggressive  \n",
      "0  ketahui informasi pembagian ppkm wilayah jabar...  \n",
      "1  tempat ibadah wilayah ppkm level berkapasitas ...  \n",
      "2  juru bicara satgas covid wiku adisasmito menje...  \n",
      "3  ketahui informasi pembagian ppkm wilayah jabar...  \n",
      "4  kementerian agama menerbitkan surat edaran nom...  \n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Processing with strategy = conservative\n",
      "\n",
      "--- Preview of conservative preprocessing results ---\n",
      "                                                text  \\\n",
      "0  Ketahui informasi pembagian #PPKM di wilayah J...   \n",
      "1  Tempat Ibadah di Wilayah PPKM Level 1 Boleh Be...   \n",
      "2  Juru bicara Satgas Covid-19, Wiku Adisasmito m...   \n",
      "3  Ketahui informasi pembagian #PPKM di wilayah J...   \n",
      "4  Kementerian Agama menerbitkan Surat Edaran Nom...   \n",
      "\n",
      "                         processed_text_conservative  \n",
      "0  ketahui informasi pembagian ppkm wilayah jabar...  \n",
      "1  tempat ibadah wilayah ppkm level boleh berkapa...  \n",
      "2  juru bicara satgas covid wiku adisasmito menje...  \n",
      "3  ketahui informasi pembagian ppkm wilayah jabar...  \n",
      "4  kementerian agama menerbitkan surat edaran nom...  \n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Processing with strategy = selective\n",
      "\n",
      "--- Preview of selective preprocessing results ---\n",
      "                                                text  \\\n",
      "0  Ketahui informasi pembagian #PPKM di wilayah J...   \n",
      "1  Tempat Ibadah di Wilayah PPKM Level 1 Boleh Be...   \n",
      "2  Juru bicara Satgas Covid-19, Wiku Adisasmito m...   \n",
      "3  Ketahui informasi pembagian #PPKM di wilayah J...   \n",
      "4  Kementerian Agama menerbitkan Surat Edaran Nom...   \n",
      "\n",
      "                            processed_text_selective  \n",
      "0  ketahui informasi pembagian ppkm wilayah jabar...  \n",
      "1  tempat ibadah wilayah ppkm level boleh berkapa...  \n",
      "2  juru bicara satgas covid wiku adisasmito menje...  \n",
      "3  ketahui informasi pembagian ppkm wilayah jabar...  \n",
      "4  kementerian agama menerbitkan surat edaran nom...  \n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Processing with gensim baseline...\n",
      "\n",
      "--- Preview of gensim baseline preprocessing results ---\n",
      "                                                text  \\\n",
      "0  Ketahui informasi pembagian #PPKM di wilayah J...   \n",
      "1  Tempat Ibadah di Wilayah PPKM Level 1 Boleh Be...   \n",
      "2  Juru bicara Satgas Covid-19, Wiku Adisasmito m...   \n",
      "3  Ketahui informasi pembagian #PPKM di wilayah J...   \n",
      "4  Kementerian Agama menerbitkan Surat Edaran Nom...   \n",
      "\n",
      "                             processed_text_baseline  \n",
      "0  ketahui informasi pembagian ppkm di wilayah ja...  \n",
      "1  tempat ibadah di wilayah ppkm level 1 boleh be...  \n",
      "2  juru bicara satgas covid 19 wiku adisasmito me...  \n",
      "3  ketahui informasi pembagian ppkm di wilayah ja...  \n",
      "4  kementerian agama menerbitkan surat edaran nom...  \n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "======================================================================\n",
      "EXPERIMENTAL EVALUATION\n",
      "======================================================================\n",
      "Training set size: 18915\n",
      "Test set size: 4729\n",
      "\n",
      "Evaluating preprocessing strategy: gensim_baseline\n",
      "gensim_baseline Accuracy: 0.8670\n",
      "gensim_baseline Balanced Accuracy: 0.7029\n",
      "gensim_baseline F1-Macro: 0.7348\n",
      "Error processing strategy gensim_baseline: 'mcc'\n",
      "\n",
      "Evaluating preprocessing strategy: standard\n"
     ]
    }
   ],
   "source": [
    "class AdvancedIndonesianTextPreprocessor:\n",
    "    \"\"\"\n",
    "    Advanced text preprocessor for Indonesian social media text using regex patterns\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.stopwords, self.stopwords_categories = (\n",
    "            get_comprehensive_indonesian_stopwords()\n",
    "        )\n",
    "        self.regex_patterns = create_regex_patterns()\n",
    "        self.negation_words = {\n",
    "            \"tidak\",\n",
    "            \"bukan\",\n",
    "            \"belum\",\n",
    "            \"jangan\",\n",
    "            \"tanpa\",\n",
    "            \"gak\",\n",
    "            \"nggak\",\n",
    "        }\n",
    "\n",
    "        self.compiled_patterns = {}\n",
    "        for name, pattern in self.regex_patterns.items():\n",
    "            self.compiled_patterns[name] = re.compile(pattern, re.IGNORECASE)\n",
    "\n",
    "    def clean_social_media_artifacts(self, text):\n",
    "        \"\"\"\n",
    "        Remove social media specific artifacts using regex\n",
    "        \"\"\"\n",
    "\n",
    "        text = self.compiled_patterns[\"urls\"].sub(\"\", text)\n",
    "        text = self.compiled_patterns[\"mentions\"].sub(\"\", text)\n",
    "        text = self.compiled_patterns[\"hashtags\"].sub(\"\", text) \n",
    "        text = self.compiled_patterns[\"phone_numbers\"].sub(\"\", text)\n",
    "        text = self.compiled_patterns[\"emails\"].sub(\"\", text)\n",
    "        text = self.compiled_patterns[\"emoticons\"].sub(\"\", text)\n",
    "\n",
    "        return text\n",
    "\n",
    "    def normalize_repeated_patterns(self, text):\n",
    "        \"\"\"\n",
    "        Normalize repeated characters and words using regex\n",
    "        \"\"\"\n",
    "\n",
    "        text = self.compiled_patterns[\"repeated_chars\"].sub(r\"\\1\\1\", text)\n",
    "        text = self.compiled_patterns[\"repeated_words\"].sub(r\"\\1\", text)\n",
    "\n",
    "        return text\n",
    "\n",
    "    def remove_noise_patterns(self, text):\n",
    "        \"\"\"\n",
    "        Remove various noise patterns using regex\n",
    "        \"\"\"\n",
    "\n",
    "        text = self.compiled_patterns[\"numbers\"].sub(\"\", text)\n",
    "        text = self.compiled_patterns[\"special_chars\"].sub(\" \", text)\n",
    "        text = self.compiled_patterns[\"single_chars\"].sub(\"\", text)\n",
    "        text = self.compiled_patterns[\"extra_whitespace\"].sub(\" \", text)\n",
    "\n",
    "        return text.strip()\n",
    "\n",
    "    def advanced_stopwords_removal(self, text, strategy=\"standard\"):\n",
    "        \"\"\"\n",
    "        Advanced stopwords removal with different strategies\n",
    "        \"\"\"\n",
    "        words = text.lower().split()\n",
    "\n",
    "        if strategy == \"standard\":\n",
    "            filtered_words = [\n",
    "                word\n",
    "                for word in words\n",
    "                if word not in self.stopwords or word in self.negation_words\n",
    "            ]\n",
    "\n",
    "        elif strategy == \"aggressive\":\n",
    "            filtered_words = []\n",
    "            for word in words:\n",
    "                if (\n",
    "                    word not in self.stopwords\n",
    "                    and len(word) > 2\n",
    "                    and word not in self.negation_words\n",
    "                ):\n",
    "                    filtered_words.append(word)\n",
    "                elif word in self.negation_words:\n",
    "                    filtered_words.append(word)\n",
    "\n",
    "        elif strategy == \"conservative\":\n",
    "            common_stopwords = (\n",
    "                self.stopwords_categories[\"common_words\"]\n",
    "                + self.stopwords_categories[\"prepositions\"]\n",
    "            )\n",
    "            filtered_words = [\n",
    "                word\n",
    "                for word in words\n",
    "                if word not in common_stopwords or word in self.negation_words\n",
    "            ]\n",
    "\n",
    "        elif strategy == \"selective\":\n",
    "            preserve_categories = [\"auxiliaries\"]\n",
    "            words_to_preserve = set()\n",
    "            for cat in preserve_categories:\n",
    "                words_to_preserve.update(self.stopwords_categories[cat])\n",
    "\n",
    "            filtered_words = []\n",
    "            for word in words:\n",
    "                if (\n",
    "                    word not in self.stopwords\n",
    "                    or word in words_to_preserve\n",
    "                    or word in self.negation_words\n",
    "                ):\n",
    "                    filtered_words.append(word)\n",
    "\n",
    "        return \" \".join(filtered_words)\n",
    "\n",
    "    def preprocess_text(\n",
    "        self, text, stopwords_strategy=\"standard\", remove_short_words=True\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Complete preprocessing pipeline\n",
    "        \"\"\"\n",
    "\n",
    "        text = text.lower()\n",
    "        text = self.clean_social_media_artifacts(text)\n",
    "        text = self.normalize_repeated_patterns(text)\n",
    "        text = self.remove_noise_patterns(text)\n",
    "        text = self.advanced_stopwords_removal(text, strategy=stopwords_strategy)\n",
    "\n",
    "        if remove_short_words:\n",
    "            text = self.compiled_patterns[\"short_words\"].sub(\"\", text)\n",
    "            text = self.compiled_patterns[\"extra_whitespace\"].sub(\" \", text).strip()\n",
    "\n",
    "        return text\n",
    "\n",
    "\n",
    "preprocessor = AdvancedIndonesianTextPreprocessor()\n",
    "\n",
    "strategies = [\"standard\", \"aggressive\", \"conservative\", \"selective\"]\n",
    "results_by_strategy = {}\n",
    "\n",
    "for strategy in strategies:\n",
    "    print(f\"\\nProcessing with strategy = {strategy}\")\n",
    "\n",
    "    processed_texts = []\n",
    "    processing_times = []\n",
    "\n",
    "    for text in df[\"text\"]:\n",
    "        start_time = time.time()\n",
    "        processed_text = preprocessor.preprocess_text(text, stopwords_strategy=strategy)\n",
    "        processing_times.append(time.time() - start_time)\n",
    "        processed_texts.append(processed_text)\n",
    "\n",
    "    results_by_strategy[strategy] = {\n",
    "        \"processed_texts\": processed_texts,\n",
    "        \"processing_times\": processing_times,\n",
    "    }\n",
    "    \n",
    "    # Display head of processed data for each strategy\n",
    "    print(f\"\\n--- Preview of {strategy} preprocessing results ---\")\n",
    "    temp_df = df.copy()\n",
    "    temp_df[f'processed_text_{strategy}'] = processed_texts\n",
    "    print(temp_df[['text', f'processed_text_{strategy}']].head())\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "\n",
    "print(\"\\nProcessing with gensim baseline...\")\n",
    "baseline_processed_texts = []\n",
    "baseline_times = []\n",
    "\n",
    "for text in df[\"text\"]:\n",
    "    start_time = time.time()\n",
    "\n",
    "    processed = remove_stopwords(text.lower())\n",
    "\n",
    "    processed = re.sub(r\"[^\\w\\s]\", \" \", processed)\n",
    "    processed = re.sub(r\"\\s+\", \" \", processed).strip()\n",
    "    baseline_times.append(time.time() - start_time)\n",
    "    baseline_processed_texts.append(processed)\n",
    "\n",
    "results_by_strategy[\"gensim_baseline\"] = {\n",
    "    \"processed_texts\": baseline_processed_texts,\n",
    "    \"processing_times\": baseline_times,\n",
    "}\n",
    "\n",
    "# Display head of baseline preprocessing results\n",
    "print(\"\\n--- Preview of gensim baseline preprocessing results ---\")\n",
    "temp_df = df.copy()\n",
    "temp_df['processed_text_baseline'] = baseline_processed_texts\n",
    "print(temp_df[['text', 'processed_text_baseline']].head())\n",
    "print(\"-\" * 80)\n",
    "\n",
    "\n",
    "def extract_tfidf_features(texts, max_features=10000, min_df=2):\n",
    "    \"\"\"\n",
    "    Extract TF-IDF features from texts\n",
    "    \"\"\"\n",
    "\n",
    "    non_empty_texts = [text if text.strip() else \"empty\" for text in texts]\n",
    "\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        max_features=max_features,\n",
    "        min_df=min_df,\n",
    "        ngram_range=(1, 2),\n",
    "        norm=\"l2\",\n",
    "        lowercase=True,\n",
    "    )\n",
    "\n",
    "    features = vectorizer.fit_transform(non_empty_texts)\n",
    "    return features, vectorizer\n",
    "\n",
    "\n",
    "def train_and_evaluate_model(X_train, X_test, y_train, y_test, model_name=\"SVM\"):\n",
    "    \"\"\"\n",
    "    Train SVM model and evaluate performance with balanced metrics\n",
    "    \"\"\"\n",
    "    model = SVC(kernel=\"linear\", random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Calculate multiple metrics for comprehensive evaluation\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    balanced_accuracy = balanced_accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    # Macro average (unweighted mean of per-class metrics)\n",
    "    precision_macro, recall_macro, f1_macro, _ = precision_recall_fscore_support(\n",
    "        y_test, y_pred, average=\"macro\"\n",
    "    )\n",
    "    \n",
    "    # Weighted average (weighted by support)\n",
    "    precision_weighted, recall_weighted, f1_weighted, _ = precision_recall_fscore_support(\n",
    "        y_test, y_pred, average=\"weighted\"\n",
    "    )\n",
    "    \n",
    "    # Per-class metrics\n",
    "    precision_per_class, recall_per_class, f1_per_class, support = precision_recall_fscore_support(\n",
    "        y_test, y_pred, average=None\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"model\": model,\n",
    "        \"accuracy\": accuracy,\n",
    "        \"balanced_accuracy\": balanced_accuracy,\n",
    "        \"precision_macro\": precision_macro,\n",
    "        \"recall_macro\": recall_macro,\n",
    "        \"f1_macro\": f1_macro,\n",
    "        \"precision_weighted\": precision_weighted,\n",
    "        \"recall_weighted\": recall_weighted,\n",
    "        \"f1_weighted\": f1_weighted,\n",
    "        \"precision_per_class\": precision_per_class,\n",
    "        \"recall_per_class\": recall_per_class,\n",
    "        \"f1_per_class\": f1_per_class,\n",
    "        \"support\": support,\n",
    "        \"predictions\": y_pred,\n",
    "    }\n",
    "\n",
    "\n",
    "def cross_validate_model(features, labels, cv_folds=5):\n",
    "    \"\"\"\n",
    "    Perform cross-validation with multiple scoring metrics\n",
    "    \"\"\"\n",
    "    model = SVC(kernel=\"linear\", random_state=42)\n",
    "    scoring_metrics = {\n",
    "        'accuracy': 'accuracy',\n",
    "        'balanced_accuracy': 'balanced_accuracy', \n",
    "        'f1_macro': 'f1_macro',\n",
    "        'f1_weighted': 'f1_weighted',\n",
    "        'precision_macro': 'precision_macro',\n",
    "        'recall_macro': 'recall_macro',\n",
    "        'roc_auc': 'roc_auc'\n",
    "    }\n",
    "    \n",
    "    cv_results = {}\n",
    "    for metric_name, metric in scoring_metrics.items():\n",
    "        try:\n",
    "            scores = cross_val_score(\n",
    "                model, features, labels, cv=cv_folds, scoring=metric\n",
    "            )\n",
    "            cv_results[metric_name] = scores\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not compute {metric_name}: {e}\")\n",
    "            cv_results[metric_name] = None\n",
    "    \n",
    "    return cv_results\n",
    "\n",
    "\n",
    "def analyze_preprocessing_effectiveness(original_texts, processed_texts, strategy_name):\n",
    "    \"\"\"\n",
    "    Analyze the effectiveness of preprocessing strategy\n",
    "    \"\"\"\n",
    "    analysis = {\n",
    "        \"strategy\": strategy_name,\n",
    "        \"avg_length_reduction\": 0,\n",
    "        \"words_removed_count\": 0,\n",
    "        \"common_removed_words\": [],\n",
    "    }\n",
    "\n",
    "    original_lengths = []\n",
    "    processed_lengths = []\n",
    "    all_removed_words = []\n",
    "\n",
    "    for orig, proc in zip(original_texts, processed_texts):\n",
    "        orig_words = orig.lower().split()\n",
    "        proc_words = proc.split()\n",
    "\n",
    "        original_lengths.append(len(orig_words))\n",
    "        processed_lengths.append(len(proc_words))\n",
    "\n",
    "        removed = set(orig_words) - set(proc_words)\n",
    "        all_removed_words.extend(removed)\n",
    "\n",
    "    analysis[\"avg_length_reduction\"] = np.mean(original_lengths) - np.mean(\n",
    "        processed_lengths\n",
    "    )\n",
    "    analysis[\"words_removed_count\"] = len(all_removed_words)\n",
    "    analysis[\"common_removed_words\"] = [\n",
    "        word for word, count in Counter(all_removed_words).most_common(10)\n",
    "    ]\n",
    "\n",
    "    return analysis\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"EXPERIMENTAL EVALUATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "\n",
    "y = df[\"sentiment\"]\n",
    "\n",
    "X_train_idx, X_test_idx, y_train, y_test = train_test_split(\n",
    "    range(len(df)), y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training set size: {len(X_train_idx)}\")\n",
    "print(f\"Test set size: {len(X_test_idx)}\")\n",
    "\n",
    "\n",
    "evaluation_results = {}\n",
    "preprocessing_analyses = {}\n",
    "\n",
    "all_strategies = [\"gensim_baseline\"] + strategies\n",
    "\n",
    "for strategy in all_strategies:\n",
    "    print(f\"\\nEvaluating preprocessing strategy: {strategy}\")\n",
    "\n",
    "    processed_texts = results_by_strategy[strategy][\"processed_texts\"]\n",
    "\n",
    "    preprocessing_analyses[strategy] = analyze_preprocessing_effectiveness(\n",
    "        df[\"text\"].tolist(), processed_texts, strategy\n",
    "    )\n",
    "\n",
    "    X_train_texts = [processed_texts[i] for i in X_train_idx]\n",
    "    X_test_texts = [processed_texts[i] for i in X_test_idx]\n",
    "\n",
    "    try:\n",
    "        features_train, vectorizer = extract_tfidf_features(X_train_texts)\n",
    "        features_test = vectorizer.transform(X_test_texts)\n",
    "\n",
    "        results = train_and_evaluate_model(\n",
    "            features_train, features_test, y_train, y_test, strategy\n",
    "        )\n",
    "\n",
    "        all_features = vectorizer.transform(processed_texts)\n",
    "        cv_results = cross_validate_model(all_features, y, cv_folds=5)\n",
    "\n",
    "        evaluation_results[strategy] = {\n",
    "            \"results\": results,\n",
    "            \"cv_results\": cv_results,\n",
    "            \"features_train\": features_train,\n",
    "            \"features_test\": features_test,\n",
    "            \"vectorizer\": vectorizer,\n",
    "            \"processing_times\": results_by_strategy[strategy][\"processing_times\"],\n",
    "        }\n",
    "\n",
    "        print(f\"{strategy} Accuracy: {results['accuracy']:.4f}\")\n",
    "        print(f\"{strategy} Balanced Accuracy: {results['balanced_accuracy']:.4f}\")\n",
    "        print(f\"{strategy} F1-Macro: {results['f1_macro']:.4f}\")\n",
    "        print(f\"{strategy} MCC: {results['mcc']:.4f}\")\n",
    "        if cv_results['balanced_accuracy'] is not None:\n",
    "            print(f\"{strategy} CV Balanced Accuracy: {cv_results['balanced_accuracy'].mean():.4f} (±{cv_results['balanced_accuracy'].std():.4f})\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing strategy {strategy}: {e}\")\n",
    "        continue\n",
    "\n",
    "\n",
    "def perform_statistical_test(baseline_scores, comparison_scores):\n",
    "    \"\"\"\n",
    "    Perform paired t-test to determine statistical significance\n",
    "    \"\"\"\n",
    "    statistic, p_value = stats.ttest_rel(comparison_scores, baseline_scores)\n",
    "    return statistic, p_value\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"STATISTICAL SIGNIFICANCE TESTING\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "if \"gensim_baseline\" in evaluation_results:\n",
    "    baseline_cv = evaluation_results[\"gensim_baseline\"][\"cv_scores\"]\n",
    "\n",
    "    for strategy in strategies:\n",
    "        if strategy in evaluation_results:\n",
    "            strategy_cv = evaluation_results[strategy][\"cv_scores\"]\n",
    "            statistic, p_value = perform_statistical_test(baseline_cv, strategy_cv)\n",
    "\n",
    "            significance = \"significant\" if p_value < 0.05 else \"not significant\"\n",
    "            print(f\"{strategy} vs Baseline:\")\n",
    "            print(f\"  t-statistic: {statistic:.4f}\")\n",
    "            print(f\"  p-value: {p_value:.4f}\")\n",
    "            print(f\"  Result: {significance}\")\n",
    "\n",
    "plt.figure(figsize=(16, 12))\n",
    "\n",
    "\n",
    "plt.subplot(3, 3, 1)\n",
    "valid_strategies = [s for s in all_strategies if s in evaluation_results]\n",
    "accuracies = [evaluation_results[s][\"results\"][\"accuracy\"] for s in valid_strategies]\n",
    "\n",
    "bars = plt.bar(\n",
    "    valid_strategies,\n",
    "    accuracies,\n",
    "    color=[\"red\", \"blue\", \"green\", \"orange\", \"purple\"][: len(valid_strategies)],\n",
    ")\n",
    "plt.title(\"Preprocessing Strategy Accuracy Comparison\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "\n",
    "for bar, acc in zip(bars, accuracies):\n",
    "    plt.text(\n",
    "        bar.get_x() + bar.get_width() / 2,\n",
    "        bar.get_height() + 0.01,\n",
    "        f\"{acc:.3f}\",\n",
    "        ha=\"center\",\n",
    "        va=\"bottom\",\n",
    "        fontweight=\"bold\",\n",
    "    )\n",
    "\n",
    "plt.ylim(0, 1)\n",
    "plt.grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "\n",
    "plt.subplot(3, 3, 2)\n",
    "# Plot multiple CV metrics for comparison\n",
    "metrics_to_plot = ['balanced_accuracy', 'f1_macro', 'accuracy']\n",
    "colors = ['blue', 'green', 'red']\n",
    "\n",
    "x_pos = np.arange(len(valid_strategies))\n",
    "width = 0.25\n",
    "\n",
    "for i, metric in enumerate(metrics_to_plot):\n",
    "    means = []\n",
    "    stds = []\n",
    "    for strategy in valid_strategies:\n",
    "        cv_data = evaluation_results[strategy][\"cv_results\"][metric]\n",
    "        if cv_data is not None:\n",
    "            means.append(cv_data.mean())\n",
    "            stds.append(cv_data.std())\n",
    "        else:\n",
    "            means.append(0)\n",
    "            stds.append(0)\n",
    "    \n",
    "    plt.bar(x_pos + i*width, means, width, label=metric, \n",
    "           color=colors[i], alpha=0.7, yerr=stds, capsize=3)\n",
    "\n",
    "plt.title(\"Cross-Validation Performance (Multiple Metrics)\")\n",
    "plt.ylabel(\"CV Score\")\n",
    "plt.xticks(x_pos + width, valid_strategies, rotation=45, ha=\"right\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "\n",
    "plt.subplot(3, 3, 3)\n",
    "avg_processing_times = [\n",
    "    np.mean(evaluation_results[s][\"processing_times\"]) for s in valid_strategies\n",
    "]\n",
    "\n",
    "plt.bar(valid_strategies, avg_processing_times, color=\"purple\", alpha=0.7)\n",
    "plt.title(\"Average Processing Time per Text\")\n",
    "plt.ylabel(\"Time (seconds)\")\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "\n",
    "\n",
    "plt.subplot(3, 3, 4)\n",
    "if preprocessing_analyses:\n",
    "    strategies_with_analysis = [\n",
    "        s for s in valid_strategies if s in preprocessing_analyses\n",
    "    ]\n",
    "    length_reductions = [\n",
    "        preprocessing_analyses[s][\"avg_length_reduction\"]\n",
    "        for s in strategies_with_analysis\n",
    "    ]\n",
    "\n",
    "    plt.bar(strategies_with_analysis, length_reductions, color=\"lightgreen\", alpha=0.7)\n",
    "    plt.title(\"Average Word Count Reduction\")\n",
    "    plt.ylabel(\"Words Removed\")\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "\n",
    "\n",
    "plt.subplot(3, 3, 5)\n",
    "if len(valid_strategies) >= 2:\n",
    "    baseline_strategy = valid_strategies[0]\n",
    "    # Use balanced accuracy as primary metric for finding best strategy\n",
    "    best_strategy = max(\n",
    "        valid_strategies, key=lambda s: evaluation_results[s][\"results\"][\"balanced_accuracy\"]\n",
    "    )\n",
    "\n",
    "    # Updated metrics to include balanced metrics\n",
    "    metrics = [\"accuracy\", \"balanced_accuracy\", \"f1_macro\", \"f1_weighted\", \"mcc\"]\n",
    "    baseline_metrics = []\n",
    "    best_metrics = []\n",
    "    \n",
    "    for metric in metrics:\n",
    "        if metric in evaluation_results[baseline_strategy][\"results\"]:\n",
    "            baseline_metrics.append(evaluation_results[baseline_strategy][\"results\"][metric])\n",
    "            best_metrics.append(evaluation_results[best_strategy][\"results\"][metric])\n",
    "\n",
    "    x = np.arange(len(baseline_metrics))\n",
    "    width = 0.35\n",
    "\n",
    "    plt.bar(x - width / 2, baseline_metrics, width, label=baseline_strategy, alpha=0.7)\n",
    "    plt.bar(x + width / 2, best_metrics, width, label=best_strategy, alpha=0.7)\n",
    "\n",
    "    plt.title(\"Performance Metrics Comparison\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    plt.xticks(x, metrics[:len(baseline_metrics)], rotation=45, ha=\"right\")\n",
    "    plt.legend()\n",
    "    plt.grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "\n",
    "plt.subplot(3, 3, 6)\n",
    "if valid_strategies:\n",
    "    # Use balanced accuracy for finding best strategy\n",
    "    best_strategy = max(\n",
    "        valid_strategies, key=lambda s: evaluation_results[s][\"results\"][\"balanced_accuracy\"]\n",
    "    )\n",
    "    best_results = evaluation_results[best_strategy][\"results\"]\n",
    "\n",
    "    cm = confusion_matrix(y_test, best_results[\"predictions\"])\n",
    "    sns.heatmap(\n",
    "        cm,\n",
    "        annot=True,\n",
    "        fmt=\"d\",\n",
    "        cmap=\"Blues\",\n",
    "        xticklabels=[\"Negative\", \"Positive\"],\n",
    "        yticklabels=[\"Negative\", \"Positive\"],\n",
    "    )\n",
    "    plt.title(f\"Confusion Matrix\\n(Best: {best_strategy})\")\n",
    "    plt.ylabel(\"True Label\")\n",
    "    plt.xlabel(\"Predicted Label\")\n",
    "\n",
    "\n",
    "plt.subplot(3, 3, 7)\n",
    "if \"gensim_baseline\" in evaluation_results:\n",
    "    baseline_acc = evaluation_results[\"gensim_baseline\"][\"results\"][\"accuracy\"]\n",
    "    improvements = []\n",
    "    strategy_names = []\n",
    "\n",
    "    for strategy in valid_strategies:\n",
    "        if strategy != \"gensim_baseline\":\n",
    "            acc = evaluation_results[strategy][\"results\"][\"accuracy\"]\n",
    "            improvement = ((acc - baseline_acc) / baseline_acc) * 100\n",
    "            improvements.append(improvement)\n",
    "            strategy_names.append(strategy)\n",
    "\n",
    "    colors = [\"green\" if imp > 0 else \"red\" for imp in improvements]\n",
    "    bars = plt.bar(strategy_names, improvements, color=colors, alpha=0.7)\n",
    "    plt.title(\"Accuracy Improvement over Baseline\")\n",
    "    plt.ylabel(\"Improvement (%)\")\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "    plt.axhline(y=0, color=\"black\", linestyle=\"-\", alpha=0.3)\n",
    "\n",
    "    for bar, imp in zip(bars, improvements):\n",
    "        plt.text(\n",
    "            bar.get_x() + bar.get_width() / 2,\n",
    "            bar.get_height() + 0.1,\n",
    "            f\"{imp:.1f}%\",\n",
    "            ha=\"center\",\n",
    "            va=\"bottom\",\n",
    "            fontweight=\"bold\",\n",
    "        )\n",
    "\n",
    "\n",
    "plt.subplot(3, 3, 8)\n",
    "if preprocessing_analyses and valid_strategies:\n",
    "    # Use balanced accuracy for finding best strategy\n",
    "    best_strategy = max(\n",
    "        valid_strategies, key=lambda s: evaluation_results[s][\"results\"][\"balanced_accuracy\"]\n",
    "    )\n",
    "    if best_strategy in preprocessing_analyses:\n",
    "        common_words = preprocessing_analyses[best_strategy][\"common_removed_words\"][\n",
    "            :10\n",
    "        ]\n",
    "        if common_words:\n",
    "            word_counts = [1] * len(common_words)\n",
    "            plt.barh(range(len(common_words)), word_counts, color=\"orange\", alpha=0.7)\n",
    "            plt.yticks(range(len(common_words)), common_words)\n",
    "            plt.title(f\"Common Removed Words\\n({best_strategy})\")\n",
    "            plt.xlabel(\"Frequency\")\n",
    "\n",
    "\n",
    "plt.subplot(3, 3, 9)\n",
    "if valid_strategies:\n",
    "    effectiveness_scores = []\n",
    "    for strategy in valid_strategies:\n",
    "        # Use a composite score: balanced accuracy + f1_macro - time penalty\n",
    "        balanced_acc = evaluation_results[strategy][\"results\"][\"balanced_accuracy\"]\n",
    "        f1_macro = evaluation_results[strategy][\"results\"][\"f1_macro\"]\n",
    "        time_penalty = np.mean(evaluation_results[strategy][\"processing_times\"]) * 0.1\n",
    "        effectiveness = (balanced_acc + f1_macro) / 2 - time_penalty\n",
    "        effectiveness_scores.append(effectiveness)\n",
    "\n",
    "    plt.bar(valid_strategies, effectiveness_scores, color=\"skyblue\", alpha=0.7)\n",
    "    plt.title(\"Overall Strategy Effectiveness\\n(Balanced Score - Time Penalty)\")\n",
    "    plt.ylabel(\"Effectiveness Score\")\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Dataset Size: {len(df)} tweets\")\n",
    "print(f\"Training Size: {len(X_train_idx)} tweets\")\n",
    "print(f\"Test Size: {len(X_test_idx)} tweets\")\n",
    "print()\n",
    "\n",
    "if evaluation_results:\n",
    "    print(\"Performance Results:\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    best_strategy = None\n",
    "    best_accuracy = 0\n",
    "\n",
    "    for strategy in valid_strategies:\n",
    "        if strategy in evaluation_results:\n",
    "            acc = evaluation_results[strategy][\"results\"][\"accuracy\"]\n",
    "            if strategy == \"gensim_baseline\":\n",
    "                print(f\"Baseline (Gensim):           {acc:.4f}\")\n",
    "                baseline_acc = acc\n",
    "            else:\n",
    "                improvement = (\n",
    "                    ((acc - baseline_acc) / baseline_acc) * 100\n",
    "                    if \"baseline_acc\" in locals()\n",
    "                    else 0\n",
    "                )\n",
    "                print(f\"{strategy:25}: {acc:.4f} ({improvement:+.2f}%)\")\n",
    "\n",
    "            if acc > best_accuracy:\n",
    "                best_accuracy = acc\n",
    "                best_strategy = strategy\n",
    "\n",
    "    print()\n",
    "    print(\"Preprocessing Analysis:\")\n",
    "    print(\"-\" * 50)\n",
    "    for strategy in valid_strategies:\n",
    "        if strategy in preprocessing_analyses:\n",
    "            analysis = preprocessing_analyses[strategy]\n",
    "            print(f\"{strategy}:\")\n",
    "            print(f\"  Average words removed: {analysis['avg_length_reduction']:.1f}\")\n",
    "            print(f\"  Total unique words removed: {analysis['words_removed_count']}\")\n",
    "\n",
    "    if best_strategy:\n",
    "        print()\n",
    "        print(\"Best Configuration:\")\n",
    "        print(\"-\" * 50)\n",
    "        print(f\"Best Strategy: {best_strategy}\")\n",
    "\n",
    "        best_results = evaluation_results[best_strategy][\"results\"]\n",
    "        print(f\"Accuracy: {best_results['accuracy']:.4f}\")\n",
    "        print(f\"Precision (Macro): {best_results['precision_macro']:.4f}\")\n",
    "        print(f\"Recall (Macro): {best_results['recall_macro']:.4f}\")\n",
    "        print(f\"F1-Score (Macro): {best_results['f1_macro']:.4f}\")\n",
    "        print(f\"F1-Score (Weighted): {best_results['f1_weighted']:.4f}\")\n",
    "        print(f\"Matthews Correlation Coefficient: {best_results['mcc']:.4f}\")\n",
    "        if best_results['auc_roc'] is not None:\n",
    "            print(f\"AUC-ROC: {best_results['auc_roc']:.4f}\")\n",
    "        \n",
    "        print(\"\\nPer-Class Performance:\")\n",
    "        for i, (prec, rec, f1, sup) in enumerate(zip(\n",
    "            best_results['precision_per_class'],\n",
    "            best_results['recall_per_class'], \n",
    "            best_results['f1_per_class'],\n",
    "            best_results['support']\n",
    "        )):\n",
    "            class_name = \"Negative\" if i == 0 else \"Positive\"\n",
    "            print(f\"  {class_name}:\")\n",
    "            print(f\"    Precision: {prec:.4f}\")\n",
    "            print(f\"    Recall:    {rec:.4f}\")\n",
    "            print(f\"    F1-Score:  {f1:.4f}\")\n",
    "            print(f\"    Support:   {sup}\")\n",
    "\n",
    "        print()\n",
    "        print(\"Computational Efficiency:\")\n",
    "        print(\"-\" * 50)\n",
    "        for strategy in valid_strategies:\n",
    "            if strategy in evaluation_results:\n",
    "                avg_time = np.mean(evaluation_results[strategy][\"processing_times\"])\n",
    "                total_time = np.sum(evaluation_results[strategy][\"processing_times\"])\n",
    "                print(f\"{strategy}: {avg_time:.6f}s/tweet, Total: {total_time:.4f}s\")\n",
    "\n",
    "        print()\n",
    "        print(\"Cross-Validation Results (Best Strategy):\")\n",
    "        print(\"-\" * 50)\n",
    "        cv_results = evaluation_results[best_strategy][\"cv_results\"]\n",
    "        for metric, scores in cv_results.items():\n",
    "            if scores is not None:\n",
    "                print(f\"{metric}: {scores.mean():.4f} (±{scores.std():.4f})\")\n",
    "\n",
    "        print()\n",
    "        print(\"Detailed Classification Report (Best Strategy):\")\n",
    "        print(\"-\" * 50)\n",
    "        print(\n",
    "            classification_report(\n",
    "                y_test,\n",
    "                best_results[\"predictions\"],\n",
    "                target_names=[\"Negative\", \"Positive\"],\n",
    "            )\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
